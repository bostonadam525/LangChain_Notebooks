{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain - Basic Integration for LLMOps\n",
    "* Notebook by Adam Lang\n",
    "* Date: 12/23/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make sure you have an `OpenAI` key saved as a .env file before running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_core in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (0.2.40)\n",
      "Requirement already satisfied: langchain_community in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (0.2.16)\n",
      "Collecting langchain_openai (from -r requirements.txt (line 3))\n",
      "  Downloading langchain_openai-0.2.14-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting langchain_chroma (from -r requirements.txt (line 4))\n",
      "  Using cached langchain_chroma-0.1.4-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting wikipedia (from -r requirements.txt (line 5))\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dotenv in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (1.0.1)\n",
      "Collecting pydantic==2.6.1 (from -r requirements.txt (line 7))\n",
      "  Downloading pydantic-2.6.1-py3-none-any.whl.metadata (83 kB)\n",
      "Collecting langfuse (from -r requirements.txt (line 9))\n",
      "  Downloading langfuse-2.57.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic==2.6.1->-r requirements.txt (line 7))\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.16.2 (from pydantic==2.6.1->-r requirements.txt (line 7))\n",
      "  Downloading pydantic_core-2.16.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.11/site-packages (from pydantic==2.6.1->-r requirements.txt (line 7)) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.11/site-packages (from langchain_core->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.11/site-packages (from langchain_core->-r requirements.txt (line 1)) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in /opt/conda/lib/python3.11/site-packages (from langchain_core->-r requirements.txt (line 1)) (0.1.129)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.11/site-packages (from langchain_core->-r requirements.txt (line 1)) (24.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.11/site-packages (from langchain_core->-r requirements.txt (line 1)) (8.5.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.11/site-packages (from langchain_community->-r requirements.txt (line 2)) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.11/site-packages (from langchain_community->-r requirements.txt (line 2)) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.11/site-packages (from langchain_community->-r requirements.txt (line 2)) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.16 in /opt/conda/lib/python3.11/site-packages (from langchain_community->-r requirements.txt (line 2)) (0.2.16)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain_community->-r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.11/site-packages (from langchain_community->-r requirements.txt (line 2)) (2.32.3)\n",
      "INFO: pip is looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain_openai (from -r requirements.txt (line 3))\n",
      "  Downloading langchain_openai-0.2.13-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Downloading langchain_openai-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Downloading langchain_openai-0.2.11-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Downloading langchain_openai-0.2.10-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading langchain_openai-0.2.9-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading langchain_openai-0.2.8-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading langchain_openai-0.2.7-py3-none-any.whl.metadata (2.6 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading langchain_openai-0.2.6-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading langchain_openai-0.2.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading langchain_openai-0.2.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading langchain_openai-0.2.3-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading langchain_openai-0.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading langchain_openai-0.2.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading langchain_openai-0.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "  Downloading langchain_openai-0.1.25-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting openai<2.0.0,>=1.40.0 (from langchain_openai->-r requirements.txt (line 3))\n",
      "  Downloading openai-1.58.1-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain_openai->-r requirements.txt (line 3))\n",
      "  Using cached tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 (from langchain_chroma->-r requirements.txt (line 4))\n",
      "  Downloading chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: fastapi<1,>=0.95.2 in /opt/conda/lib/python3.11/site-packages (from langchain_chroma->-r requirements.txt (line 4)) (0.115.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.11/site-packages (from wikipedia->-r requirements.txt (line 5)) (4.12.3)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.4.0 in /opt/conda/lib/python3.11/site-packages (from langfuse->-r requirements.txt (line 9)) (4.6.2.post1)\n",
      "Collecting backoff>=1.10.0 (from langfuse->-r requirements.txt (line 9))\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: httpx<1.0,>=0.15.4 in /opt/conda/lib/python3.11/site-packages (from langfuse->-r requirements.txt (line 9)) (0.27.2)\n",
      "Requirement already satisfied: idna<4.0,>=3.7 in /opt/conda/lib/python3.11/site-packages (from langfuse->-r requirements.txt (line 9)) (3.10)\n",
      "Requirement already satisfied: wrapt<2.0,>=1.14 in /opt/conda/lib/python3.11/site-packages (from langfuse->-r requirements.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r requirements.txt (line 2)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r requirements.txt (line 2)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r requirements.txt (line 2)) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r requirements.txt (line 2)) (1.15.5)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio<5.0.0,>=4.4.0->langfuse->-r requirements.txt (line 9)) (1.3.1)\n",
      "Collecting build>=1.0.3 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4))\n",
      "  Using cached build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4))\n",
      "  Using cached chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (0.32.0)\n",
      "Collecting posthog>=2.4.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4))\n",
      "  Downloading posthog-3.7.4-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4))\n",
      "  Downloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (1.27.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4))\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4))\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (1.27.0)\n",
      "Requirement already satisfied: tokenizers<=0.20.3,>=0.13.2 in /opt/conda/lib/python3.11/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (0.15.2)\n",
      "Collecting pypika>=0.48.9 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4))\n",
      "  Using cached PyPika-0.48.9-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.11/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (4.66.5)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.11/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.11/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (6.4.5)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /opt/conda/lib/python3.11/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (1.62.2)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /opt/conda/lib/python3.11/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (4.2.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.11/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (0.12.5)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4))\n",
      "  Using cached kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4))\n",
      "  Using cached mmh3-5.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /opt/conda/lib/python3.11/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (3.10.7)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.11/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (13.9.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community->-r requirements.txt (line 2)) (3.23.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community->-r requirements.txt (line 2)) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.41.0,>=0.37.2 in /opt/conda/lib/python3.11/site-packages (from fastapi<1,>=0.95.2->langchain_chroma->-r requirements.txt (line 4)) (0.40.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1.0,>=0.15.4->langfuse->-r requirements.txt (line 9)) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1.0,>=0.15.4->langfuse->-r requirements.txt (line 9)) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0,>=0.15.4->langfuse->-r requirements.txt (line 9)) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain_core->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from langchain<0.3.0,>=0.2.16->langchain_community->-r requirements.txt (line 2)) (0.2.4)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai->-r requirements.txt (line 3)) (1.9.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain_openai->-r requirements.txt (line 3))\n",
      "  Downloading jiter-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain_community->-r requirements.txt (line 2)) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain_community->-r requirements.txt (line 2)) (1.26.19)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain_community->-r requirements.txt (line 2)) (3.1.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain_openai->-r requirements.txt (line 3)) (2024.9.11)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4->wikipedia->-r requirements.txt (line 5)) (2.5)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4))\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (2.9.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (2.35.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (1.8.0)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4))\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4))\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4))\n",
      "  Using cached durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4))\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (4.25.3)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (1.13.3)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (6.10.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (1.65.0)\n",
      "Collecting grpcio>=1.58.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4))\n",
      "  Downloading grpcio-1.68.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4))\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4))\n",
      "  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4))\n",
      "  Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4))\n",
      "  Downloading protobuf-5.29.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4))\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4))\n",
      "  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4))\n",
      "  Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4))\n",
      "  Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4))\n",
      "  Using cached asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4))\n",
      "  Downloading opentelemetry_api-1.29.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4))\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (2.18.0)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /opt/conda/lib/python3.11/site-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (0.25.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.11/site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.11/site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (1.5.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community->-r requirements.txt (line 2)) (1.0.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (0.6.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (0.24.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (13.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>=3.8.3->langchain_community->-r requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (4.9)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (2023.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (3.20.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (0.1.2)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4))\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma->-r requirements.txt (line 4)) (0.6.1)\n",
      "Downloading pydantic-2.6.1-py3-none-any.whl (394 kB)\n",
      "Downloading pydantic_core-2.16.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_openai-0.1.25-py3-none-any.whl (51 kB)\n",
      "Using cached langchain_chroma-0.1.4-py3-none-any.whl (10 kB)\n",
      "Downloading langfuse-2.57.1-py3-none-any.whl (254 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading chromadb-0.5.23-py3-none-any.whl (628 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "Downloading openai-1.58.1-py3-none-any.whl (454 kB)\n",
      "Using cached tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Downloading jiter-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
      "Using cached kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
      "Using cached mmh3-5.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
      "Downloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m132.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.29.0-py3-none-any.whl (55 kB)\n",
      "Downloading grpcio-1.68.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m144.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl (30 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl (166 kB)\n",
      "Downloading opentelemetry_api-1.29.0-py3-none-any.whl (64 kB)\n",
      "Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl (6.9 kB)\n",
      "Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl (118 kB)\n",
      "Downloading posthog-3.7.4-py2.py3-none-any.whl (54 kB)\n",
      "Using cached durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading protobuf-5.29.2-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=b5f549679e879cfff4cf9e7491fac8671a2c123d9655bdff4973e305cac248fa\n",
      "  Stored in directory: /home/sagemaker-user/.cache/pip/wheels/8f/ab/cb/45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: pypika, monotonic, durationpy, pyproject_hooks, pydantic-core, protobuf, opentelemetry-util-http, oauthlib, mmh3, jiter, humanfriendly, grpcio, chroma-hnswlib, backoff, asgiref, annotated-types, wikipedia, tiktoken, requests-oauthlib, pydantic, posthog, opentelemetry-proto, opentelemetry-api, coloredlogs, build, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, openai, onnxruntime, langfuse, kubernetes, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langchain_openai, opentelemetry-instrumentation-fastapi, chromadb, langchain_chroma\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.62.2\n",
      "    Uninstalling grpcio-1.62.2:\n",
      "      Successfully uninstalled grpcio-1.62.2\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.17\n",
      "    Uninstalling pydantic-1.10.17:\n",
      "      Successfully uninstalled pydantic-1.10.17\n",
      "  Attempting uninstall: opentelemetry-api\n",
      "    Found existing installation: opentelemetry-api 1.27.0\n",
      "    Uninstalling opentelemetry-api-1.27.0:\n",
      "      Successfully uninstalled opentelemetry-api-1.27.0\n",
      "  Attempting uninstall: opentelemetry-semantic-conventions\n",
      "    Found existing installation: opentelemetry-semantic-conventions 0.48b0\n",
      "    Uninstalling opentelemetry-semantic-conventions-0.48b0:\n",
      "      Successfully uninstalled opentelemetry-semantic-conventions-0.48b0\n",
      "  Attempting uninstall: opentelemetry-sdk\n",
      "    Found existing installation: opentelemetry-sdk 1.27.0\n",
      "    Uninstalling opentelemetry-sdk-1.27.0:\n",
      "      Successfully uninstalled opentelemetry-sdk-1.27.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker 2.227.0 requires protobuf<5.0,>=3.12, but you have protobuf 5.29.2 which is incompatible.\n",
      "tensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.7.0 asgiref-3.8.1 backoff-2.2.1 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.23 coloredlogs-15.0.1 durationpy-0.9 grpcio-1.68.1 humanfriendly-10.0 jiter-0.8.2 kubernetes-31.0.0 langchain_chroma-0.1.4 langchain_openai-0.1.25 langfuse-2.57.1 mmh3-5.0.1 monotonic-1.6 oauthlib-3.2.2 onnxruntime-1.20.1 openai-1.58.1 opentelemetry-api-1.29.0 opentelemetry-exporter-otlp-proto-common-1.29.0 opentelemetry-exporter-otlp-proto-grpc-1.29.0 opentelemetry-instrumentation-0.50b0 opentelemetry-instrumentation-asgi-0.50b0 opentelemetry-instrumentation-fastapi-0.50b0 opentelemetry-proto-1.29.0 opentelemetry-sdk-1.29.0 opentelemetry-semantic-conventions-0.50b0 opentelemetry-util-http-0.50b0 posthog-3.7.4 protobuf-5.29.2 pydantic-2.6.1 pydantic-core-2.16.2 pypika-0.48.9 pyproject_hooks-1.2.0 requests-oauthlib-2.0.0 tiktoken-0.8.0 wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "\n",
    "# OPENAI_API_KEY=\n",
    "dotenv.load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Setup\n",
    "* First we need to set up the core prompts we will use for a basic LLM call to the Open AI API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI bot. Your name is Bob.'), HumanMessage(content='Hello, how are you doing today?'), AIMessage(content=\"I'm doing well, thanks!\"), HumanMessage(content='What is your name?')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate ## prompt templates\n",
    "\n",
    "## There are 4 basic personas for the ChatPromptTemplate below\n",
    "# 'human', 'user', 'ai', 'assistant', or 'system'\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "            (\"human\", \"Hello, how are you doing today?\"),\n",
    "            (\"ai\", \"I'm doing well, thanks!\"),\n",
    "            (\"human\", \"{user_input}\"),\n",
    "        ])\n",
    "\n",
    "## invoke final prompt \n",
    "## invoke or async_invoke are the usual invoke methods\n",
    "final_prompt = prompt.invoke({\n",
    "        \"name\": \"Bob\",\n",
    "        \"user_input\": \"What is your name?\"\n",
    "    })\n",
    "\n",
    "## print final_prompt\n",
    "final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI bot.'), HumanMessage(content='Hi!'), AIMessage(content='How can I assist you today?'), HumanMessage(content='Can you make me an ice cream sundae?'), AIMessage(content='No.')])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder ## define a place holder variable\n",
    "\n",
    "\n",
    "## example using MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI bot.\"),\n",
    "    MessagesPlaceholder(variable_name=\"conversation\", optional=True)\n",
    "])\n",
    "\n",
    "# Conversation can be retireved from other sources like Vector store, other APIs\n",
    "final_prompt = prompt.invoke(\n",
    "    {\n",
    "        \"conversation\": [\n",
    "            (\"human\", \"Hi!\"),\n",
    "            (\"ai\", \"How can I assist you today?\"),\n",
    "            (\"human\", \"Can you make me an ice cream sundae?\"),\n",
    "            (\"ai\", \"No.\")\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "final_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup LLM via OpenAI API\n",
    "* Setup connection to LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 14, 'total_tokens': 44, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-830ae7ca-479c-4f00-acbc-47c534c5fe89-0', usage_metadata={'input_tokens': 14, 'output_tokens': 30, 'total_tokens': 44})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "## init the LLM of choice\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "## invoke the LLM with a zero shot question\n",
    "llm.invoke(\"Hello, how are you doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm here and ready to help you. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 14, 'total_tokens': 32, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_9faba9f038', 'finish_reason': 'stop', 'logprobs': None}, id='run-db7b9a90-54c3-461b-afcb-059fd6df362c-0', usage_metadata={'input_tokens': 14, 'output_tokens': 18, 'total_tokens': 32})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using a different model\n",
    "## init the new llm model\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "## invoke same zero shot input\n",
    "llm.invoke(\"Hello, how are you doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 14, 'total_tokens': 44, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_5f20662549', 'finish_reason': 'stop', 'logprobs': None} id='run-c95c5889-bd62-4cd6-b0a1-8c7539ac2faa-0' usage_metadata={'input_tokens': 14, 'output_tokens': 30, 'total_tokens': 44}\n",
      "\n",
      "\n",
      "content=\"Hello! I'm just a virtual assistant, so I don't have feelings, but I'm here and ready to help you with whatever you need. How can I assist you today?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 14, 'total_tokens': 49, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_d28bcae782', 'finish_reason': 'stop', 'logprobs': None} id='run-ab989fef-1ff8-419c-8090-6452f1b476da-0' usage_metadata={'input_tokens': 14, 'output_tokens': 35, 'total_tokens': 49}\n"
     ]
    }
   ],
   "source": [
    "# different Hyperparameters\n",
    "## temp=0 is deterministic\n",
    "print(ChatOpenAI(model=\"gpt-4o\", temperature=0, max_tokens=100).invoke(\"Hello, how are you doing?\"))\n",
    "print(\"\\n\")\n",
    "## temp=2 is probabilistic\n",
    "print(ChatOpenAI(model=\"gpt-4o\", temperature=2, max_tokens=100).invoke(\"Hello, how are you doing?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Output Parser\n",
    "* We can change the structured output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Different Output Parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = llm | StrOutputParser()\n",
    "chain.invoke(\"Hello, how are you doing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains\n",
    "* This is the most basic use case of LangChain for LLMOps using chains.\n",
    "* It is also one of the most common use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My name is Bob. How can I assist you today?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1. Setup prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "            (\"human\", \"Hello, how are you doing?\"),\n",
    "            (\"ai\", \"I'm doing well, thanks!\"),\n",
    "            (\"human\", \"{user_input}\"),\n",
    "        ])\n",
    "## 2. Setup LLM and parser\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, max_tokens=100)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "## 3. Create Chain using LCEL syntax\n",
    "chain = (prompt \n",
    "         | \n",
    "        llm \n",
    "         | \n",
    "        parser)\n",
    "\n",
    "## 4. invoke chain\n",
    "chain.invoke({\n",
    "        \"name\": \"Bob\",\n",
    "        \"user_input\": \"What is your name?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "My\n",
      " name\n",
      " is\n",
      " Bob\n",
      ".\n",
      " How\n",
      " can\n",
      " I\n",
      " assist\n",
      " you\n",
      " today\n",
      "?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Streaming invocation\n",
    "stream = chain.stream({\n",
    "        \"name\": \"Bob\",\n",
    "        \"user_input\": \"What is your name?\"\n",
    "})\n",
    "\n",
    "\n",
    "for token in stream:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading chain\n",
    "* What we see below is we can save the chain in a separate json file and load it at runtime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load import dumpd\n",
    "import json\n",
    "\n",
    "json_str_chain = dumpd(chain)\n",
    "with open(\"./chain.json\", \"w\") as fp:\n",
    "    json.dump(json_str_chain, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note about below:\n",
    "* The chain is smart enough to not save our API key secrets to the json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'openai_api_key': 'OPENAI_API_KEY'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.middle[0].lc_secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the chain below from the json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['name', 'user_input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], template='You are a helpful AI bot. Your name is {name}.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Hello, how are you doing?')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"I'm doing well, thanks!\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], template='{user_input}'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fa73d5f3b90>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fa73d95b5d0>, root_client=<openai.OpenAI object at 0x7fa73d42fa10>, root_async_client=<openai.AsyncOpenAI object at 0x7fa73d5f14d0>, model_name='gpt-4o', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=100)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.load import load\n",
    "import os\n",
    "\n",
    "with open(\"./chain.json\", \"r\") as fp:\n",
    "    chain = load(json.load(fp), secrets_map={\"OPENAI_API_KEY\": os.getenv(\"OPENAI_API_KEY\")})\n",
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can invoke the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My name is Bob. How can I assist you today?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "        \"name\": \"Bob\",\n",
    "        \"user_input\": \"What is your name?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
